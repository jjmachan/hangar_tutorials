{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangar Tutorial (2/2): Training a Model using the Data in Hangar\n",
    "#### Now lets make some models with the data we put in hangar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hangar\n",
    "from hangar import Repository\n",
    "from hangar import make_torch_dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangar.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets continue from where we left off. \n",
    "\n",
    "All the data from MNIST has now been added successfully to the Hangar repo. Lets load the repo and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repository('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Contents Contained in Data Repository \n",
      " \n",
      "================== \n",
      "| Repository Info \n",
      "|----------------- \n",
      "|  Base Directory: /home/jjmachan/jjmachan/hangar_tutorial \n",
      "|  Disk Usage: 105.88 MB \n",
      " \n",
      "=================== \n",
      "| Commit Details \n",
      "------------------- \n",
      "|  Commit: a=39a36c4fa931e82172f03edd8ccae56bf086129b \n",
      "|  Created: Fri May  1 18:23:19 2020 \n",
      "|  By: jjmachan \n",
      "|  Email: jjmachan@g.com \n",
      "|  Message: added all the mnist datasets \n",
      " \n",
      "================== \n",
      "| DataSets \n",
      "|----------------- \n",
      "|  Number of Named Columns: 6 \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_test_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_test_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_training_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 50000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_training_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 50000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_validation_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_validation_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it shows the columns and specifications of how they are stored internally in hangar along with size, dtype information.\n",
    "\n",
    "To access let's create a read-only checkout from the master branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: master with current HEAD: a=39a36c4fa931e82172f03edd8ccae56bf086129b\n"
     ]
    }
   ],
   "source": [
    "# Create a Read checkout\n",
    "co = repo.checkout(branch='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hangar provides two Dataloaders to import the data stored in the Hangar repositories directly for training in Tensorflow (*make_tf_dataset*) or PyTorch(*make_torch_dataset*). Both these take a list of columns and return a dataset with each index values in the columns.\n",
    "\n",
    "The `make_torch_dataset` returns a PyTorch `Dataset` and that means we can use the `DataLoader` provided by PyTorch which makes it super simple for loading the data, splitting it into batches etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train, test and val datasets using\n",
    "# th make_torch dataset in hangar. This takes the \n",
    "# columns and creates a torch dataset out of it.\n",
    "\n",
    "train_dataset = make_torch_dataset((co['mnist_training_images'], co['mnist_training_labels']))\n",
    "test_dataset = make_torch_dataset((co['mnist_test_images'], co['mnist_test_labels']))\n",
    "val_dataset = make_torch_dataset((co['mnist_validation_images'], co['mnist_validation_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that currently Hangar does not seem to support multiple workers for the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Neural Network with 3 layers and outputs the logits. Takes in input shape (in this case 784) and the output shape (in this case 10).\n",
    "\n",
    "Now I won't be explaining the details of the model or the training loops but if all this seems new to you I highly recomment you checking out [this](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, inShape, outShape):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(inShape, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, outShape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.fc1(input))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# initialize the model\n",
    "model = net(784, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 0/10] Train Loss: 1.2083537247954312\n",
      "Test Loss: 0.47797256113050846 Accuracy: 0.865814696485623\n",
      "[EPOCH 1/10] Train Loss: 0.3944594695549208\n",
      "Test Loss: 0.3467818654228609 Accuracy: 0.897064696485623\n",
      "[EPOCH 2/10] Train Loss: 0.31767420198765994\n",
      "Test Loss: 0.2960374093682954 Accuracy: 0.9107428115015974\n",
      "[EPOCH 3/10] Train Loss: 0.27709063706813486\n",
      "Test Loss: 0.2613061714274719 Accuracy: 0.9223242811501597\n",
      "[EPOCH 4/10] Train Loss: 0.24662601495887404\n",
      "Test Loss: 0.234231408689909 Accuracy: 0.9306110223642172\n",
      "[EPOCH 5/10] Train Loss: 0.22161395768786918\n",
      "Test Loss: 0.21181030162928488 Accuracy: 0.9365015974440895\n",
      "[EPOCH 6/10] Train Loss: 0.20021527176466666\n",
      "Test Loss: 0.19286749035137862 Accuracy: 0.9421924920127795\n",
      "[EPOCH 7/10] Train Loss: 0.18172580767267993\n",
      "Test Loss: 0.1767021114335428 Accuracy: 0.946685303514377\n",
      "[EPOCH 8/10] Train Loss: 0.16573666792806246\n",
      "Test Loss: 0.16299303889887545 Accuracy: 0.9507787539936102\n",
      "[EPOCH 9/10] Train Loss: 0.1518441192202165\n",
      "Test Loss: 0.15127997121999795 Accuracy: 0.954073482428115\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss_test = 0\n",
    "    total_loss_train = 0\n",
    "    accuracy = 0\n",
    "    for img, label in train_loader:\n",
    "        label = label.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "    for img, label in test_loader:\n",
    "        label = label.view(-1)\n",
    "        with torch.no_grad():\n",
    "            # Train Loss\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            total_loss_test += loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, indx = out.topk(1)\n",
    "            correct = (indx.view(-1) == label).sum().item()\n",
    "            acc = correct/batch_size\n",
    "            accuracy += acc\n",
    "    \n",
    "    # Print losses for each epoch\n",
    "    train_loss = total_loss_train/len(train_loader)\n",
    "    test_loss = total_loss_test/len(test_loader)\n",
    "    accuracy = accuracy/len(test_loader)\n",
    "    print(f'[EPOCH {epoch}/{epochs}] Train Loss: {train_loss}')\n",
    "    print(f'Test Loss: {test_loss} Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and voilà!\n",
    "\n",
    "we have trained a simple neural network with the data stored in hangar. Now you have all the tools to successfully train models from your own data stored in Hangar.\n",
    "\n",
    "Cheers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DataLoader Speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a auxiliary section which compares the speed of the dataloaders using Hangar and a simple dataloader we have written.\n",
    "\n",
    "Now lets try to see if there is any speedup in using the DataLoaders from Hangar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset unzips the mnist data file and loads the data directly into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mnist_file_path, split='train'):\n",
    "        assert split in ('train', 'test', 'val')\n",
    "        self.split = split\n",
    "        \n",
    "        # Load data to memory\n",
    "        with gzip.open(mnist_file_path, 'rb') as f:\n",
    "            train_set, val_set, test_set = pickle.load(f, encoding='bytes')\n",
    "        if split == 'train':\n",
    "            self.imgs = train_set[0]\n",
    "            self.labels = train_set[1]\n",
    "        if split == 'test':\n",
    "            self.imgs = test_set[0]\n",
    "            self.labels = test_set[1]\n",
    "        elif split == 'val':\n",
    "            self.imgs = val_set[0]\n",
    "            self.labels = val_set[1]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = mnist_dataset('./mnist.pkl.gz', 'train')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our handwritten case the mnist_dataset is loading the data file into memory, creating the dataloader and iterting through all of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7ff66c6430d0\n",
      "0x7ff66c64fc90\n",
      "0x7ff66c6439d0\n",
      "0x7ff66c64f710\n",
      "0x7ff66c6434d0\n",
      "0x7ff66c64f250\n",
      "0x7ff66c643cd0\n",
      "0x7ff66c64fa10\n",
      "1.16 s ± 63.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dataset = mnist_dataset('./mnist.pkl.gz', 'train')\n",
    "# print the memory location to see if during each test the \n",
    "# the dataset is getting actually loaded to a new loc in memory.\n",
    "print(hex(id(dataset)))\n",
    "dataloaders = DataLoader(dataset, batch_size=32)\n",
    "for img, label in dataloaders:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.21 s ± 87.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "for img, label in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a bummer, the hangar dataloaders seems to be almost 7x slower that our simple custom dataloader. \n",
    "\n",
    "Also we try to manually feed data for training using hangars getitem but gives similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_col = co['mnist_training_images']\n",
    "label_col = co['mnist_training_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7 s ± 192 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for i in range(len(img_col)):\n",
    "    img = torch.from_numpy(img_col[i])\n",
    "    label = torch.from_numpy(label_col[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.17 s ± 48.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with img_col, label_col:\n",
    "    for i in range(len(img_col)):\n",
    "        img = torch.from_numpy(img_col[i])\n",
    "        label = torch.from_numpy(label_col[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
