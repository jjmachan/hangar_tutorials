{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangar Tutorial (2/2): Training a Model using the Data in Hangar\n",
    "#### Now lets make some models with the data we put in hangar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hangar\n",
    "from hangar import Repository\n",
    "from hangar import make_torch_dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hangar.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets continue from where we left off. \n",
    "\n",
    "All the data from MNIST has now been added successfully to the Hangar repo. Lets load the repo and see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repository('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Contents Contained in Data Repository \n",
      " \n",
      "================== \n",
      "| Repository Info \n",
      "|----------------- \n",
      "|  Base Directory: /home/jjmachan/jjmachan/hangar_tutorial \n",
      "|  Disk Usage: 105.88 MB \n",
      " \n",
      "=================== \n",
      "| Commit Details \n",
      "------------------- \n",
      "|  Commit: a=39a36c4fa931e82172f03edd8ccae56bf086129b \n",
      "|  Created: Fri May  1 18:23:19 2020 \n",
      "|  By: jjmachan \n",
      "|  Email: jjmachan@g.com \n",
      "|  Message: added all the mnist datasets \n",
      " \n",
      "================== \n",
      "| DataSets \n",
      "|----------------- \n",
      "|  Number of Named Columns: 6 \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_test_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_test_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_training_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 50000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_training_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 50000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_validation_images\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (784,) \n",
      "|    - dtype: float32 \n",
      "|    - backend: 00 \n",
      "|    - backend_options: {'complib': 'blosc:lz4hc', 'complevel': 5, 'shuffle': 'byte'} \n",
      "|\n",
      "|  * Column Name: ColumnSchemaKey(column=\"mnist_validation_labels\", layout=\"flat\") \n",
      "|    Num Data Pieces: 10000 \n",
      "|    Details: \n",
      "|    - column_layout: flat \n",
      "|    - column_type: ndarray \n",
      "|    - schema_hasher_tcode: 1 \n",
      "|    - data_hasher_tcode: 0 \n",
      "|    - schema_type: fixed_shape \n",
      "|    - shape: (1,) \n",
      "|    - dtype: int64 \n",
      "|    - backend: 10 \n",
      "|    - backend_options: {} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it shows the columns and specifications of how they are stored internally in hangar along with size, dtype information.\n",
    "\n",
    "To access let's create a read-only checkout from the master branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Checking out BRANCH: master with current HEAD: a=39a36c4fa931e82172f03edd8ccae56bf086129b\n"
     ]
    }
   ],
   "source": [
    "# Create a Read checkout\n",
    "co = repo.checkout(branch='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hangar provides two Dataloaders to import the data stored in the Hangar repositories directly for training in Tensorflow (*make_tf_dataset*) or PyTorch(*make_torch_dataset*). Both these take a list of columns and return a dataset with each index values in the columns.\n",
    "\n",
    "The *make_torch_dataset* returns a PyTorch Dataset and that means we can use the DataLoader provided by PyTorch which makes it super simple for loading the data, splitting it into batches etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train, test and val datasets using\n",
    "# th make_torch dataset in hangar. This takes the \n",
    "# columns and creates a torch dataset out of it.\n",
    "\n",
    "train_dataset = make_torch_dataset((co['mnist_training_images'], co['mnist_training_labels']))\n",
    "test_dataset = make_torch_dataset((co['mnist_test_images'], co['mnist_test_labels']))\n",
    "val_dataset = make_torch_dataset((co['mnist_validation_images'], co['mnist_validation_labels']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note is that currently Hangar does not seem to support multiple workers for the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Neural Network with 3 layers and outputs the logits. Takes in input shape (in this case 784) and the output shape (in this case 10).\n",
    "\n",
    "Now I won't be explaining the details of the model or the training loops but if all this seems new to you I highly recomment you checking out [this](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, inShape, outShape):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(inShape, 500)\n",
    "        self.fc2 = nn.Linear(500, 200)\n",
    "        self.fc3 = nn.Linear(200, outShape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.fc1(input))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# initialize the model\n",
    "model = net(784, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 0/10] Train Loss: 0.06814072782266706\n",
      "Test Loss: 0.0918073209914596 Accuracy: 0.9716453674121406\n",
      "[EPOCH 1/10] Train Loss: 0.06410053524654061\n",
      "Test Loss: 0.08948530110020261 Accuracy: 0.9723442492012779\n",
      "[EPOCH 2/10] Train Loss: 0.06032143013524422\n",
      "Test Loss: 0.08731502429254877 Accuracy: 0.9728434504792333\n",
      "[EPOCH 3/10] Train Loss: 0.056838085246138315\n",
      "Test Loss: 0.08546964528010087 Accuracy: 0.9732428115015974\n",
      "[EPOCH 4/10] Train Loss: 0.05359196147025799\n",
      "Test Loss: 0.08372617617901117 Accuracy: 0.9734424920127795\n",
      "[EPOCH 5/10] Train Loss: 0.050556435092588885\n",
      "Test Loss: 0.08219187844603415 Accuracy: 0.9737420127795527\n",
      "[EPOCH 6/10] Train Loss: 0.04774323870436546\n",
      "Test Loss: 0.08069135308440123 Accuracy: 0.9740415335463258\n",
      "[EPOCH 7/10] Train Loss: 0.04511407855756626\n",
      "Test Loss: 0.07942048499828201 Accuracy: 0.974341054313099\n",
      "[EPOCH 8/10] Train Loss: 0.04266405477471857\n",
      "Test Loss: 0.07819276037175929 Accuracy: 0.9745407348242812\n",
      "[EPOCH 9/10] Train Loss: 0.04034396769629073\n",
      "Test Loss: 0.07719437592308045 Accuracy: 0.974341054313099\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss_test = 0\n",
    "    total_loss_train = 0\n",
    "    accuracy = 0\n",
    "    for img, label in train_loader:\n",
    "        label = label.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_train += loss.item()\n",
    "        \n",
    "    for img, label in test_loader:\n",
    "        label = label.view(-1)\n",
    "        with torch.no_grad():\n",
    "            # Train Loss\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            total_loss_test += loss.item()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, indx = out.topk(1)\n",
    "            correct = (indx.view(-1) == label).sum().item()\n",
    "            acc = correct/batch_size\n",
    "            accuracy += acc\n",
    "    \n",
    "    # Print losses for each epoch\n",
    "    train_loss = total_loss_train/len(train_loader)\n",
    "    test_loss = total_loss_test/len(test_loader)\n",
    "    accuracy = accuracy/len(test_loader)\n",
    "    print(f'[EPOCH {epoch}/{epochs}] Train Loss: {train_loss}')\n",
    "    print(f'Test Loss: {test_loss} Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and voilà!\n",
    "\n",
    "we have trained a simple neural network with the data stored in hangar. Now you have all the tools to successfully train models from your own data stored in Hangar.\n",
    "\n",
    "Cheers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare DataLoader Speeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a auxiliary section which compares the speed of the dataloaders using Hangar and a simple dataloader we have written.\n",
    "\n",
    "Now lets try to see if there is any speedup in using the DataLoaders from Hangar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset unzips the mnist data file and loads the data directly into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mnist_file_path, split='train'):\n",
    "        assert split in ('train', 'test', 'val')\n",
    "        self.split = split\n",
    "        \n",
    "        # Load data to memory\n",
    "        with gzip.open(mnist_file_path, 'rb') as f:\n",
    "            train_set, val_set, test_set = pickle.load(f, encoding='bytes')\n",
    "        if split == 'train':\n",
    "            self.imgs = train_set[0]\n",
    "            self.labels = train_set[1]\n",
    "        if split == 'test':\n",
    "            self.imgs = test_set[0]\n",
    "            self.labels = test_set[1]\n",
    "        elif split == 'val':\n",
    "            self.imgs = val_set[0]\n",
    "            self.labels = val_set[1]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = mnist_dataset('./mnist.pkl.gz', 'train')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = DataLoader(dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our handwritten case the mnist_dataset is loading the data file into memory, creating the dataloader and iterting through all of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x7ff66c6430d0\n",
      "0x7ff66c64fc90\n",
      "0x7ff66c6439d0\n",
      "0x7ff66c64f710\n",
      "0x7ff66c6434d0\n",
      "0x7ff66c64f250\n",
      "0x7ff66c643cd0\n",
      "0x7ff66c64fa10\n",
      "1.16 s ± 63.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "dataset = mnist_dataset('./mnist.pkl.gz', 'train')\n",
    "# print the memory location to see if during each test the \n",
    "# the dataset is getting actually loaded to a new loc in memory.\n",
    "print(hex(id(dataset)))\n",
    "dataloaders = DataLoader(dataset, batch_size=32)\n",
    "for img, label in dataloaders:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.21 s ± 87.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "for img, label in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can see the hangar dataloaders seems to be almost 7x slower that our simple custom dataloader. \n",
    "\n",
    "I took a small peak into the source code and it seems like it could be due to the extra overhead in getting one single element from Hangar (remember the context managers issues) but then I'm not sure and will update if I get the developers word on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
